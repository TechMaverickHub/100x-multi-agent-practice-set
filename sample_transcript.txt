Hey, guys. Can you hear me? (clears throat) Can you see me? Let me know in the chat. Awesome. Oh, excited to learn MCP. Yes. Hey, Amber. Hey, Shahir. Let's- let's go. Hey, Jeff. What's up? Um, so today's session gonna be more hands-on. I thought like ... Oh, there are two ways to do this session. Either I will share my screen and do all the things for you guys, which again, uh, puts a lot of pressure on me. So I thought, why don't we divide the pressure? And I basically like, uh, give couple of tasks to you guys. I'll still walk you through the steps. Um, first we're gonna implement a tool call in the same order what we learned yesterday, like how can you integrate a third-party tool with an LLM, what are the steps you need. I've also recorded a video about the same thing, so if you have not watched that video, you first need to watch that video. That would really help you to grasp the concepts and, uh, yeah, we're gonna like implement that first. Once you learn how do you integrate APIs with LLMs, then we're gonna learn how to build MCP. And then you will see by implementing these two things like what is the difference and, uh, where can you like apply things. I have a couple of interesting MCPs with me, uh, they're use cases, which, um, maybe like I've- I think you guys might find it useful, so we're gonna talk about those things at the end of the session. But the first part is gonna be like more on the hands-on and- and building side of things. So yeah, before we start, let's do a quick live check. It's been a long time. Um, yeah. How are you feeling? Not built 5-coded. Yeah, uh, Gary built something very interesting. I- I just checked that. Yeah. How's, uh, how's this project coming along with you? You mentioned it's like running in production, right? I think you should share the post also with everyone. Oh, wow. Even Kate mail- gateway also integrated. Nice. Awesome. 8.2. Cool. Uh, let me share my screen and we're gonna start. As I mentioned, for the tool calling activity, you don't need to install a lot of third-party libraries right now. Um, I'll just like walk you through the process. So we will first integrate the tool just like what we discussed yesterday, if you remember that diagram. So this was a problem statement with which we started the activity and the lesson also. And we realized that you guys are not able to like finish this with the Groq playground. So what we're gonna do today is like first we fix this problem with our Groq. Uh, you can feel free to use any model. Um, I'm gonna use the OpenAI GPT-OSs for that. And we'll try to solve this entire loop and implement this, uh, logic as well, what we discussed yesterday. And then we move to the MCP version. So first thing first, we need to install Python locally. Uh, I have a couple of files already with me, so I'll quickly just walk you through the steps. And right after that, I'll start a timer. So you guys need to build that thing, uh, live right now. And then we'll do a fastest finger first, whoever builds it first, I'll, uh, invite them and we basically like share some insights. Let me go and first check my app. Okay. This is my... Can you guys see the font size? Is font size fine? I think I'm- I just zoomed in. Let me know if you can see the- or read e- everything clearly. Yeah, I think Shahir, uh, the same thing what Nimesh mentioned is a- is a answer. Most of the good models, like even especially the 20 billion parameter model of GPT-OSs, if you try to run locally, it- first of all, two things will happen. Your computer will start getting slow because it will consume a lot of RAM, so it has to like allocate some virtual RAM to this particular model in order for it to run an inference. And then another thing is, even if you have good hardware, it's just not the best use of your resources because at the end of the day, you will be running a quantized model locally. And what is this quantized model? Uh, you can think of right now as a compressed version of it, not the actual model, because most of the model which you run through OLAMA and all these services, they are gonna be like a slightly compressed versions, not the actual versions. So-Yeah, like this is one thing which I- I've been, like, trying to experiment a lot where I run models locally, but e- every time I do that, I just, like, don't get the best output. Um, and yeah. And it's, it's... Plus, most of the credit for Groq API is free, so why do our engineer and make our life harder, right? Unless until you have a specific use case, then I understand what... Yeah, that's the thing. So in the meanwhile, should we create a new project and activate? Yes, exactly. So thanks for sharing this. I- I was about to tell the same thing. Just create a new project, create one Python file and, uh, yeah, maybe set up your virtual environment. Make sure everything is done. I'll walk you through the s- current steps. Like, I'll actually just remove and compress everything so far. So let's try to map everything, what we have learned in the yesterday's lecture. And if you understand these concepts correctly, I think you will be able to build any kind of system which can implement function calling. So first step is identifying the LLM. And I mention, in this case, our LLM gonna be... I'm gonna use GPT-OS's, specifically 20 billion version, uh, from Groq. So first things first, if you don't have a Groq API key, go to Groq, de- get the API key and export it on your CLI if you want to just, like, use it without using the .env. If you want to use .env file, then you have to install the package and extract the key. So step number one is installing the GPT-OS's 20 billion parameter model. Um, how do we do that? If you have attended the previous sessions, I think by this time, you already know. First, you get the API key. So I'm getting this API key from my terminal, um, but you can also get it from your .env. And this is where the response... This is the main model call. So this is the LLM layer. I'll keep adding the tags as well. So this is the LLM... Okay. I added a couple of other things which you can remove for now if you don't need it, like, um, the default parameters also work here, so you can just keep it to the bare minimum as well. But this is something which we need. Even I can remove this for now because we are building things from scratch. All of you familiar with this API call where we use chat completions, mention the model, mention the message, and this is where we start our, uh, exercise in the implementation. We implement what is the weather like in Bengaluru, which was the same query or kind of similar query which we are asking to the LLM. So step number one, build this part, add the query, and set up things locally, which- with your model. I'll also mention this as step one. Any confusion in this? I don't think you guys should have confusion. If you have confusion, I think you should watch the previous lectures because nothing new we have done here. Um, only... Which is... I've done here is to print the Groq response also. So just, when you're running the application, make sure at least this part of your code is working so we can check this box. So our step number one, which is LLM setup, is done. How many of you have done this? Can we have a quick raise of hand, um, so that I can see, like, how, how many of you are doing it along with me? Okay, we have a couple of folks who are already doing it. If I get more than five hand raised, then I will move to the next part. So I'll wait for the other folks as well. I want all of you to implement this right now. Create a simple Python file. Um, create this, uh, Groq response API call and, uh, try to run it. See if you can just quickly set this part up. This is like a boilerplate setup only. Well, nice. We, we got more than five people. Oh. Um, cool. I'll just keep five as a threshold right now. Step number two. What would be the step number two? We have to take this query and we have to first find the tool, right? So there are two ways to implement this. One, I will directly go with the tool approach. Um, I'm gonna go to the APIs, implement those APIs, and then connect this with our LLM. Right now, I'm going to show you a simpler approach, but this approach is also gonna help you build a template with you, just like what we discussed in the previous sessions. If you have a template which works for a generic use case and then you use this template to add, like, specific things for your future use cases, this will gonna help you keep a library of your code bases which worked with you in the past. So anytime you want to start a new project, instead of asking Claude or Cursor to start things from scratch, which has a chance that they might hallucinate, you can use these templates. So I'm going to first tell you the template approach which can help you get, connect with any other tool.Um, so I'm gonna just create a simple function. So this function will take the same. I'll mock the, uh, OpenWeatherMap tool. It will take the input and give, give some output back. But right now this function is just a Python function to not make things complicated and, uh, once we implement this entire part, tomorrow you can add anything in this Python function and you can extend this to, uh, I would say any, any type of tool integration. So e- even before like, uh, doing, uh, anything else, so I'll start by writing function. Now one, one thing which is there, I have not mentioned where is our wrapper or the execution environment. Can anyone guess where is this? We have done the step number one with LLM setup. Now I'm moving towards building this tool, tool slash function for this, which we will be calling. But we haven't built a wrapper. Did we build a wrapper? How many of you have this question? Can you send the starting code? Um, all the code is on this repository by the way. Like if you wanna get the code, let me see. By the way, I have done all, all these things in the previous session, so that's why I didn't share everything. Um... So if you go to groq-function-call, this is the repository. Actually don't use directly the function call one, just use, uh, API. I think groq-API is the one, if I'm not wrong. Yeah, this is the one. I'll send this to all of you. Cool, I sent you the code so people who are falling behind, you can just like use this, set up the groq LLM and uh... I think somebody asked, response will be shown in the terminal. Yes, response will be shown in the terminal. We don't have any UI right now so it's just terminal-based application. Cool. Hmm. Yeah. So let me come back. What I was asking is that second step for us is to integrate tool or define the tool. But we skipped one layer here, we implemented the LLM part with the code, and now we are gonna define this tool or the function, but we missed this part and that was my question that did we miss this part or not? And if yes, then where is the wrapper? This is a quick test to all of you to see how much you have retained from the past lectures. "Wrapper is supposed to be around the LLM." Yes. We, we discussed this part where we knew that all the LLMs are just this lossy compressed version of file. Even the LLM which you are gonna be using from groq API, it's gonna just give you this model inference. It does not have any other tools what ChatGPT has because ChatGPT has built their own wrapper that is called ChatGPT. So, but you guys have also learned like how to build those things. So my question was like, yeah, where is this wrapper? Did we build this wrapper? If not, then should we? Okay, I think now I'm getting some question. You might- you guys need to think in this entire session, I'm gonna put more pressure on you guys. And if you feel overwhelmed, I think it's okay. This is a part of learning. And uh, you just take notes. Whenever you feels something is confusing, something is like not making sense to you, take a note and you can send all these notes to me after the lecture also. I'm happy to like, uh, clear all these doubts there. Because sometimes it, it might not, say, uh, make sense right now in the lecture but if you send it to me I can maybe give you better examples. But right now the question which I ask that where is the execution layer? Where is the wrapper that we supposed to buil- build around the LLM? And uh, I think some people got the gist of it. The wrapper is... Wrapper is not the API, Nabil. Uh, API is the LLM. Wrapper is our Python file. So this entire file which we have built, don't you think this file is actually a wrapper? Even the code which I have sent to all of you guys. Let me find this code. So this, let's, let's do one quick visualization because I know for some people who already understand this concept, this would be like a repetitive thing but I want to clarify to everyone because I want most people to complete this tool calling exercise after this. Let's see and compare this part with our... Oops. Hang on.In this case, we are building this Python file. This will be some .py file. Now what this .py file is doing, it is actually using a model and this model is your, maybe in this case, like Mixtral but you can use like GPT-OS. By the way, Mixtral is like outdated. This code is like some one-year-old code so you, you need to change this particular model name. But this is the model, this entire thing. And everything around this is the wrapper. Did you get this point? Because this model is running here, so whatever output this model will give, we give the input here. This is where we give the input, the content part, and we receive some output from this chat completions variable and we can then use this variable to execute the code. Now this is also your execution environment and this execution environment is actually Python environment because it's a Python script. So, the difference between this versus you running on Groq cloud, what you were doing yesterday if you remember, is that here you are just calling the API and it is giving you the answer of, uh, your input and output which is similar to you just talking to this API directly here and you don't have anything else apart from that. But if I want to call a particular API or run the code, because we know that LLM can, uh, write code but here, we can't execute the code and this is true for all the APIs if you use the models via API or just model here. So you can't run the code. Now this is where our execution environment helps. So we can ask the LLM to generate code and once we got the code back from the LLM, we already running everything in the Python environment so we can execute those code. Do you see the execution environment and how Python file what we have created is becoming that? And this hence, this is our wrapper application because this is very fundamental concept of the entire session. If you don't understand this, I don't think anything will make sense after this point to you. Is this clear? Any doubts on this? Awesome. Yes. Cool. Let's move to the second part then. So we finished very quickly two things. We're done with the LM layer. We're also done with the wrapper layer by just putting this in our Python file. And now the final part here is we need to define the function. Now this function should take input and output and then it, we need to connect it. So first, let's define the tool. This is step number two, which we're gonna do. So, now this is where the step number two is coming. I'll actually change this to two. Define the function to fetch the context. If you remember yesterday, we were discussing about like how can we stop the hallucination or avoid it by providing the relevant context. So this context here could be anything. That's why this template is very important because you can tomorrow use the same template to fetch any kind of context and you can like solve your particular hallucination problem also here. Um, we'll define a Python function just like what we've been doing so far in the full stack module. Uh, we, you get some input here. Now, this is important. Input could be anything but we are building this specifically for the weather, to, to solve the weather query, so the input is city name. Um, here it says written location but, um, I'll just like keep this location for now what, you can add the city name here. Now what this function does, it actually is a starting function which means like it doesn't do any processing. This is a sample weather data so I have written location and temperature and location is the same location which is coming in from the, uh, I would say... Should I change it to city name? Let me change this to city name for now. Actually, let me finish the first one with location and then you guys will change the city n-... I don't want to like change in the entire code base other wise it'll throw me off. But just like, uh, for now map this location to your city name. It's the same part. What we are doing is taking the input, processing it. Here it's a dummy processing but you can use it to call your APIs and then once you get the response back which is this weather, so it's a JSON response and we return the, uh, data also in the JSON so we convert this into JSON dump and return the weather data. This data is always gonna send you 50 year temperatures. Now, this is starting data. Why we are doing this? Just to create a template. Nothing fancy so far. Created a function, defined the input, added a processing layer, this is a dummy processing layer right now, and returned the output. Output, input, function. We have input, we have output, we have a function. The step number two is done. How many of you have followed along? Step number one and step number two?You can also take a screenshot of this and just, like, copy this exact thing. Maybe you can change the location to your city name. Awesome. Co- it, it's gonna be, like, quick step. Uh, I, I don't wanna wait for, like, all of you guys to finish because I know it's just one single function call. For now, don't worry about, like, adding API call. If you just, like, figure out the input/output. We'll first build all these th- three layers and then later on we can add the API calls. Okay. Now, we have, uh, step number three. Step number three is connecting the tool with LLM. Because we have our LLM, we have our tool, now the connection part is remaining. Now here we discussed that how LLMs are great at understanding the natural language, so we can leverage their natural language processing capability and find the inputs from there. And then we can extract the information from here and maybe write an API call or API call equivalent or a function call equivalent in this case, and call this function and get the output back. And then we can figure out, like, how to use the output. We can combine this output with the LM and then share it with, uh, the user, the response. Or we can just, like, store the output somewhere, whatever logic you wanna do. Now in order for us to do that there are multiple ways and one of the major problem with OpenAI solved, and I was mentioning this with the tool calling approach, uh, before OpenAI released the tool calling people still were trying to do this but via prompting. And you can actually try this out today if you wanna, if you wanna figure out how do you engineer a function calling from scratch. You can ask model to write, uh, I would say a function code and also extract the information. So it should look like that whenever you s- receive a query like this you define the job of the LM. Step number one, extract the information which is equivalent to city name because we need city name. And maybe you can use few short learning here. You give examples, this is the example which you can get from the user. This is the output you should get. So this is where your prompt engineering connecting the dot, um, y- like coming here where you can, you can train the model which is, which is a way how people used to do it. The problem with that way is because LMs are non-deterministic in nature, every time your model change or your model gives a slightly different response, your code will break because here two things are happening. Now this is, this is very important part which very less people understand and this is also another reason why most of the LLM application in production today are not stable and it has more bugs than normal software because we are trying to combine two systems. So this one system, which is a Python file, the wrapper, it is very deterministic. In the sense, like, if you have written client is equals to Groq and, uh, if you have missed this bracket or even if you have missed a particular colon here, your code will not run. So your code is 100% deterministic and the problem here is your model, which is GPT-OSS, this is, this is completely opposite. This is, like, totally undeterministic. So everything which is coming out of this m- this output could be, like, different but everything what you want to write here or w- what you need to write here is, is gonna be, like, always fixed. That was a problem which a lot of people faced because you cannot rely on the prompting technique. So couple of solutions. Now this recently, like, uh, got popular with the, a new solution that OpenAI released called Structured Output. If you guys don't know about Structured Output, it is a problem that we are discussing right now that how can we make model more deterministic in nature. So with Structured Output wha- what OpenAI did is that they fine-tuned this model to give very specific type of output. Now these output could be JSON, could be XML, um, so you can define like what kind of structured output you want. If you wanna take a look at how Structured Output works you can also look at Groq Cloud and if you click on this settings parameter you can... I think if I go here maybe they have... Okay, I think JSON mode is there. So they have not specifically defined a Structured Output, but if you use the JSON mode it- it does the same thing. Let me test it also. So as you can see, like, right now it's giving me the warning that you must instruct the model to use JSON when using this model. Um, so... So as you can see the output here, if you write the same thing without JSON mode... Let me open Groq here right now and I'll... Yeah, JSON mode is off.Same model, but see the difference here. Uh, it gives me first the- the text, then it gives me like a lot of things here. It gives me some comments as well, then yeah, it made a couple of things extra here. Then feel free to drop all these things. Now problem is, I cannot use this entire text as it is. I have to first filter the string and use my string processing, if you have done the BigBinary exercises of string manipulation. This is where you need to apply those skills and ex- exactly specify this. And that was also tedious task, because sometimes you have like multiple variables, you have to like understand the output. It's just like very hectic. And because LMs are again non-deterministic in nature, so it can always give you new output, so you cannot cover all the edge cases. Now this is a problem with structured output solved, in this case, the JSON mode, uh, for Groq Playground. If you go to, uh, the Playground for GPT, there you can find the structured output as our keyword. But what it does, it just like forces your LM to give you exactly JSON response. Nothing else, no strings, no, uh, extra information, exactly what you want. Because this was fine-tuned to give answers like that. Now why this is helpful, because this solved a very important problem of non-deterministic and communicating between these two systems, which are completely different in nature. By doing this, LLMs became more, the output of LM became more deterministic in a structured JSON format, and I can trust that I can use this exact JSON. For example, in this case, if I gave this, uh, query, uh, I can directly use this in my Python code, and I can copy this and create a Python function, maybe say def, like, API_call, and I can directly call this. Maybe like send this as a response. So it will most probably like give me the right response, if I have mentioned all the parameters and everything correct. This was a main logic how, uh, OpenAI solved the tool calling, and later on they put it out in the structured output as well. Like this became like a separate API. People started using it for all- all sort of other things as well apart from tool calling. But that was like a crux of it. So they did fine-tune the model to achieve this functionality, and uh, yeah, to solve this nature of deterministic, non-deterministic nature of these two systems. Um, along with that, because this structured output thing is great, we also need to define that, how can we extract the information? Because this has, this is only solving the output part. Whenever I ask it to generate output, it will give me the JSON, but what about the input part? Because I need to also tell the LM that if somebody's writing a query like this, then you need to give the city name. Eh, this is the only relevant information we have. So how do you mention this part? Now this is where again prompting came into the picture, where, uh, you would define the- the- the prompts here. Which again became the same problem. The input become non-deterministic. In order for us to solve this part, we gave the structured input. Now because structured output is also in JSON, we said okay, let's put the structured input also in JSON, because we're gonna use the same language and we're gonna reduce all the ambiguity in the LMs. So we know that if this is the out, eh, input which has a weather field, then this should be the output. This is how we are Python programmer, mostly deterministic programs work. So we take this approach and we package this as... You can call it tool calling or function calling. This is the programmatic tool calling I'm- I'm talking about, not the conceptual, what we learnt yesterday. But this is how exactly the implementation of tool calling works today. You have a structured input in a JSON format, you have a structured output in a JSON format. And why do we have these two things? Because we are dealing with two different systems where, which have two different, uh, nature of the output. So this solves us the unification of communication problem. Now once you have this, you need to start defining. So there would be a one case where we put- put the input to the model, and we extract the output. So let's implement that part right now, and that is our step number three, what I was discussing from last 10, five, 10 minutes, is how do we connect the tools with LM? And I hope you got the reason that why it was not straightforward that you ca- you can just write a prompt and just maybe like, uh, yeah, make some API calls. Cool. If this is clear, we'll... No, JSON is not the only way. It's just JSON is the standard way in the REST API world, so if you have seen all the APIs, their response is in JSON format. So it was easier decision for OpenAI or any other model to use the most widely used, uh, API response or request format so that they don't need to j- like, uh, change a lot of things. That was another decision which they took. But yeah, you can, uh, find structured outputs in different formats also, and you can like do it for XML, for maybe in your own programming language also. People are doing that right now. But cool, let's now implement the step number three. So we go here, we have our step number one, the LLM, and then we define the function, which was a step number two. And then as we know, we have... Let me do...Step number three. And step number 3.1 first, which is structured input (pause) Now in the structured input, this is based on OpenAI documentation. This is how you describe a tool. So, actually I will just remove this entire thing. We'll keep it clean. So we define the structured input for our LLM to, to use it as a tool. Now you'll, you'll realize that everything what we ask model or what we expect model to take as an input, we mention it here. So because location was a, a input to our function, so we defined the properties. So this, this entire format is basically written by OpenAI so that everyone can follow this format where you first define the function, and because this entire thing is a function to LLM. So whenever the model... So model was fine-tuned and it was trained in such a way that whenever it discover that type is equals to function, it will start behaving differently, which means it will not take these things as regular prompt. It will try to understand that this is a structured input which is supposed to do a certain job, not your regular LLM task. And, uh, then in order for you to describe the entire function, you mention the name of the function. Uh, this is get current weather. Now this name will help LLM to pick this particular part and whenever it is using and calling it. So it refer this function as a name. And then the description, now this is a prompt actually, so it is a very important prompt. I would say this in, in the entire structured input, if you write this prompt clearly, your LLM will be able to understand when to pick this call. I think somebody, somebody has this doubt yesterday is that how does LLM figure out that which part of the, uh, input I need to pick and then how to extract information? It basically like takes from here. Right now the input is straightforward so that's why I written just get the current weather in a given location. This is how the function works, by the way. Uh, so this is a function description. And then parameters, type of the parameters, properties, and then these are two properties which is actually one property only, uh, which is the location. So location has a type. I'm defining it because it could be a number also, it could be a string, it could be image. So I'm just restricting it to string right now. And description is this is the city and the state. And if anything is required or optional, you mention here. So because I want location is the only property here, so this should be required. This is like a structured input, how it looks like in program. And once you define this, you have your first part of the third step ready. Now you need, uh, you won't need to like do specifically define the structured output because this is a feature of the LLM, just like what you saw on the Groq cloud. The moment you toggle this particular thing out, which is the JSON mode, you can start getting the structured output back. So you don't have to do a lot of, uh, say programming for this. In, in this application, how does that look like? This is where, um... Actually I think... Yeah. So this is how it looks like where if you mention the tools here, now this is also another part. Everything what you want to send LLM2 as, as a tool, you mention here and you wrap this entire thing in a Python variable. So you pass it down. You can have multiple tools. This is like a first tool. Um, so this is the first function. You can have like multiple functions. Functions could be like your, uh, after this particular block you can define like one more function that could have a, a, I would say, I think yeah, after this particular part. So just like this has name, I can do next could be so get current temperature or get current, uh, maybe stock market data or anything. Like stock would be like a different kind of function all together. So you can add like multiple functions here. But again, just be aware of the same problem which I discussed with you guys yesterday, that you will see like a, uh, I would say a problem, but I wanted you guys to experience that problem first, e- even before me sharing this thing out. But, um, so from the implementation part out, this was the last step which we need to do where we define the structured input. Again, you can take a screenshot of this or you can just like go to my GitHub repository. I have all the code there. And, uh, you define the tools here. So you connect... This was a three point- two part. So 3.1 was defining the tool, uh, structured input for the tool, and then connecting it and passing it to the same LLM API call. And then you can add a tool choice. So if you put auto tool choice then LLM will decide based on the prompt which you have written. You can also programmatically change it here. For example, if you know exactly this particular script which you have written is only going to need one particular function, so you can change and you can like add your, um, t- tool name here, which could be, say, get weather function.But in this case I will keep it auto. And anyways, it has only one function so it won't make any difference right now. With this, we've finished our entire program and we also completed all the three steps which we need in order for us to implement the LLM layer, the wrapper layer, and also the function layer. Now you would say, um, "What's the difference between this, uh, versus the actual application?" And I'll actually try to run this right now. Let me see if I can run this. Oh, API key. Okay. So let me export my API key. Actually, I found something interesting. Like earlier I used to blur my screen and, like, hide. I found this extension which can add a blur and I can still add my API keys without showing you guys. Uh... Okay. So now our function is running, everything what we have written here. The last part, I think, which I have not explained is we got the response, and I'll walk you through the response, uh, as well. This was... Let me open this terminal in full screen. So first, let's notice what is happening here. So chat completion is the API call which we sent, and this is the entire content. Everything which is, uh, we are receiving as a response from the Groq Playground, um, is basically printing here. The executed tools, none. First, like, reasoning need to, uh, need to call the get current function. So this was a reasoning which it basically caught, uh, with the Bengaluru, which was correct based on the structured input. Tool calls, this was a tool call which it caught. So it identified the tool, uh, call with the two parameters which we defined in the function call. Those were actually only one parameter location and the type was string, so it has mentioned everything in string. So this entire part was the input, what it caught and understood from the query, which was, "What is the weather like in Bengaluru?" And then this was the output, which is location Bengaluru and temperature 50. Now, why temperature 50? Because this is what we have written in our function, which is written here. So as of now, we can validate that our input and output is working fine. We have defined a function, we have defined a structured input for the LLM, and we have added the LLM layer and also mentioned the tools which helps us to put the structured output back. How many of you have completed this step? Let's do a quick hand raise, because I know I have been speaking a lot while I was explaining this. Okay, cool. I think, uh, again, if you guys got lost, um, and anyone who is basically following along, you will face some issues, you will face some errors. It's gonna be completely fine. Um, you have to, like, add and fix those issues. Only then you will build that muscle memory with you that these are the steps. But I'll quick, do a quick recap before I move to the next part. Uh, although, although I'm running short on time so I, I won't spend too much time. I've recorded a video to do the exact same thing which I'm doing right now, but me typing all these things, so you can watch that video also. So don't worry if you don't, like, get a chance to, like, follow along with this. Uh, but there will be bu- bugs, there will be issues, which is completely fine and it's part of the process. But let m- let me do a quick recap of what we have done, so then we'll move to the next part. "Code without errors is an imagination." Yes. So we are right now seeing my imagination, which, which was so far working fine. Okay. Let me put this... We wanted to implement these three layers, the LLM layer, the function layer, and the execution environment, the wrapper layer. Step number one, we did the LLM setup. In order for us to do LLM setup, we have to create a file. And when we create a file and set up the LLM, this step entirely gives you two part, which is the LLM layer, that is your, uh, the first layer here in this part, and then the second part which is your execution layer, that is your Python file. Once you have this, you can generate code and you can also execute this code in the same environment.... which solves your very important problem which Groq Playground or, or raw LLM cannot do. Then second thing which we need in order for us to define a function or call any particular third-party software tool, we need to define that particular function. And every function takes some input, some output. It could be singular, it could be multi- multiple. In this case, it takes city name as the input and output as the temperature, so we have to define this in our code. What we did, we first went and we added the LLM layer. This was not there initially. Sorry, this is how the code was there. We added the LLM layer and then we defined the se- this is the second part, which is define the tool. To get the fetch context, this is the input, this is the output. This is also important part for this entire step, so you define the city name as the input. And this entire part, I'm doing as an output, which gives you location and the temperature. And I'm sending this as a JSON, so we're done with the second part. Now third part is where we got to deal with few ne- nitty-gritty details, or you would say nuances of how LLMs work. We cannot directly, uh, wire these two things together because the code has a deterministic nature. Everything what you write here is gonna execute exactly. Even if you miss one single bracket or column, uh, it won't run. On the other side, LLMs are very flexible. They can, like... even if you write a wrong, say, natural language prompt, it will still give you answer because they give answers based on prediction and the token as we have learnt it. But the drawback of this approach, being very flexible, is that the output become very non-deterministic. So you cannot, like, predict that what is the output every single time. This was a problem we face and which lot of other developers also face. To solve this problem, we came up with two approach. Um, actually one approach. This is, uh, coined by and suggested by OpenAI, that we fine-tune this model and we say that you're gonna, for a spec- special s- cases, you wanna give the output, uh, in a particular format. Now why JSON? Because JSON is the widely used language in communicating between machine to machine in the APIs. So they pick, okay, something which is widely popular. Let's pick JSON as a defined language. And if this model gives me output all the time with the JSON output, and this is what we saw here as well, you turn on the JSON mode, it doesn't give you anything. So no bullshit, just pure code. And this was a great step because now we can predict the output. So every single time, I know that exactly this is how it can give me, uh, the output. Once we got this clarity, this is great as a feature. So OpenAI implemented this in the APIs. A lot of other providers like Groq, they implement this in their APIs. So we solved the problem of getting a deterministic output from a non-deterministic... actually, I said undeterministic. It should be non-deterministic. Sorry, my bad. So from a non-deterministic, um, LLM, we got deterministic output. Now we solved the output problem, but we have still the input problem because input s- supposed to be, uh, right now in the natural language. This is what we have given, the what is the weather in Bengaluru, so we have to convert this input in a structured input so that it can understand, just like a program. And we're gonna use the same language, what we are using for the output, JSON. Uh, and if you define everything in the structured input JSON, which is... This is how it looks like. So you... And this entire thing is designed by OpenAI. That's what I was es- telling you yesterday. It is a JSON-RPC protocol layer. Actually, implementation, not a protocol. This was JSON-RPC, uh, uh, I would say layer, where they have mentioned everything like this. So if you make changes here, by the way, if you maybe, say, misspell the function name or misspell the type and name other things, there might be a chance that it won't work properly. In some cases, it will still work because LLMs are getting smarter day by day. But back in the day when I was testing things out, a- almost one year back, even if you miss, like, a small part here, it just, like, gives you some weird outputs sometimes. So what they did is that they fine-tuned the model to accept a certain kind of input and they defined this schema. So we said that if you want to follow this approach and you can define all your functions which you want to connect to the large language model, in this case it could be Weather, it could be, like, a doc, it could be Google Drive, you have to define it in a structured input format because this is how you are expecting also. So we defined it. This is the name. This is the description. This is, this will tell you, this will tell the LLM that why and when to use this function. So the description here is, "Get the current weather in a given location." So if LLM can reason that this query user is asking me about weather, and then it can look at the description of all the tools and find, okay, this description is matching with what user is asking, the intent, this is the intent mapping part, then it can select this function, look at what are the properties, uh, which are the input here. So input is only location and which is a required input, so... And this should be a type of string. So now it will try to extract the location from the LLM input, which is this part. And, uh, once it has extracted the input, then it can give you the response back. That's basically how we define the structured input part for the LLM integration. So we got, for the step number three, (sniffs) we defined the structured input in the JSON format, mentioned all the required parts, and then finally, we connected... This is 3.2 step after defining the input, the only thing which you need to do to make sure that LLM will take this input and pass it-... just like, uh, any other function in Python, and add this, uh, parameter. Right now I'm keeping tool choice auto. Um, and once we do all of these things, the response which you will get from here, you can print this response, and this is what I did. So... This is a response which I got d- directly from the, uh, Groq completion, is that you have a role, uh, this was the reasoning, need to call get w- get a function, it figured this part out, and then it added all the things. Uh, this is where it got the location correct, and once it got the location, so the next part, this is the output where I'm printing it. So y- if you see like one hack which I did here, now, this is a hack by the way, this is not the correct way of doing it, I missed the last step of our entire part. Did you notice what is the part which I missed here in the implementation? So I told you how to extract the information. We used structured input from LLM, connected the LLM, built the Python application, the wrapper, add a function, add input, add output, and then we basically take the input and send it to the user back. This part I have not implemented yet. So I've only finished this until this, where it is extracting the input and calling the function and getting the output back. This is what it is doing on the CLI also. If you can see, this was the input, this was the output. So it extracted the input and output, it is there in this field. I think we have, if you have to s- visualize this is, until this part we have implemented everything, but this is the part which is missing. I have not added this input, sorry, the output with the original prompt, and then sent it back to the LLM. This is the part which is missing. This was intentionally missing because this is something which I want you guys to do. Otherwise, like, what's the point of me doing this exercise with you? If everything I will implement, then you're just copy pasting my code. So this is the part for all of you to do, and I'll give you exactly five minutes right now to implement this last part. Um, and let's see if anyone can finish this. If you are somebody who has followed all the parts and watched the pre-recorded video which I uploaded, I think you should be able to do it right now. But even if you have not watched it, it's, it's still good to try things out right now. I think it will just help you build that muscle. I'll quickly repeat what you're supposed to do. I've implemented the entire layer and the implementation from getting the input and taking the output back in this file. I'm gonna share this file with all of you guys, um, let me... So, take this, you need to change, uh, the model name here, because this is deprecated so you can use the GPT-3. But everything else you can keep it same. The code should work. And then you have to export the Groq API key. So take this, even if you haven't written all the code, just copy this code, run it on your local machine, and your task is to implement and take this output, send it back to LLM. I want a natural language response, not the response which I am getting here, which is a JSON response. So I want this final step to be implemented. This is what we supposed to do right now. I'll start a quick five-minute timer. If you finish this before five minute, then it's great. Yeah, you can also add the weather API, that's, that's the other part which I have also missed. But I have actually done this in a different file. Okay, meanwhile you guys are doing this and they're implementing the last part, I will cu- I will complete my code because you are right, I missed two part, one part is calling the actual weather API. Let me see if I can, I can also do that in, in next five minutes with you guys. This is the older code which I had, and it's using the OpenWeather map API now. This is the API which I got, and how, how did I figure out this API? Because I told you yesterday, um, OpenWeather... If you go to their API section, you can find the API docs, and this is where you will see all the API and how to make an API call to this particular service. And I'm gonna use the same API call and you can also do the, uh, same part where this is the URL, then you get the response and, uh, you get the, the weather data also.... in the output. So this is the output which will get the temperature, feels like, humidity, pressure, wind, description. It gives a lot of information. Um, and then I just, like, added a try catch. This was an extra thing which I did. Now, why try catch? Because whenever you make a network call, it is a- a third party call, right? And sometimes you might not have control over a third party API call because maybe tomorrow OpenWeatherMap API shuts down or they are facing some crisis in their company, uh, so they don't have enough budget to basically keep running all this, or- or maybe, like, some crash happened in their facility. A lot of things can happen which is not under your control. In that case, your, if you don't add an exception handling, uh, part here, your code will also break and it is dependent on someone else's API. So this is a bit best practice. Whenever you add anything which is third party, uh, which is like a, something which can go down or can raise some exceptions or have some errors, you wrap this entire thing around try catch blocks. That's what I did here. And whenever fails because of any of the error, it throws the error down. That- that's like a just b- good practice to implement it. That's the only thing which you need to do in order for you to actually connect our template function with an actual weather API function. That's why I told you that this template is very important, because just like how you connected today with weather API u- using the same format, you can now connect this to any other API. You can just change the API URL here. It could be stock market, it could be your own personal application, what you have built, or it could be, like, anything in the world. As long as that thing has an API, you can connect it with your LLM as a tool, and then you can, like, get the input and output. Let me run this also and see if I can get a response. Hmm. I'm not sure if I have the o- open weather API key. Wait 10... Okay, the API key is working. So this time I got the output, which is, like, a longer output and I got the temperature which was not 50 degrees Celsius. It's 23.667 degree, which is the correct temperature right now. So as you can see, now we are using the function call. So, yeah, I think that's... We are almost done with the timer also. Anyone has completed this? You have last 15 seconds, or 10 seconds right now. Oh, nice. Ronak has done that. Cool, your time is over right now but, uh, I want, uh, Ronak, because you are the winner of the fastest finger first, to quickly showcase what you have done and maybe, like, tell everyone, um, about your implementation. Can you unmute? Yeah. Hi, Siddhant. Hey. So I'll just share my screen. Yep. Is my cursor visible? Yes. Yeah. So first, we had created the normal response, but then I just, uh, uh, put this prompt here in cursor to put it in another LL- LLM call and return the response in text format. So it created another, uh, response here. Like, it, uh, took the response from the first call and put it in second call and created a system and user and then it gave me the output in this format. Oh, nice. Awesome. Um, so this is your dummy function right now but... Which one? Yeah, yeah, dummy function. Okay. I haven't integrated the weather API key yet. Okay, nice. I think this is great. Um, can you just quickly show the part where you have added the output to the LLM? Can you scroll up? Okay, so here you got the weather data and- Yeah. ... uh, and then you used this. Oh, you're parsing the weather data also and then you're- Yeah, it is going into the same. ... this weather dictionary and then sending it in a prompt. The role will be user. Please format this, uh, weather data. So you're combining the same thing, so you can format this part, right? Um, I would say, like, don't- don't you think, like, this would be slightly, um, different because- Since there are longer (techno music plays) Yeah. Y- you don't have the history here. So what is missing here, I- I would say, you should add a previous message also in the chat so that it knows what user has a- asked. Can you scroll up? What was the first message? Yeah. So you can actually add this message also, this entire block. Um- Okay. Yeah. Because this is one other way how you keep the context history. Right? Like, uh, when you use ChatGPT or any wrapper application, they use this feature where they keep on adding your previous conversation so that you don't need to, like, think about, uh, yeah, should I copy paste my past chat or not. So I think you can add those things. Oth- otherwise, it might, uh, feel unnatural. Uh, but yeah. Yeah, will do that. Awesome. Yeah, this is great, Ronak. Uh, thank you for sharing this.Cool. Uh, I think if you guys got the idea of what we are doing today, you now have a very important skill. The skill is connecting LLM to any other tool in the world. But as you know, yesterday we discussed couple of problems also with this skill. Uh, one is, I would say you should try right after this, uh, lecture. You guys have added one tool. Try to add, like, multiple tools and s- and test your LM output after each and every tool. For example, um, just like what Rana did today, he has added the tool and maybe you can add the API calls because then it would be much more real rather than using a template function call. So you can use this particular file. Again, this file is also available on GitHub so you can, uh, check this entire file. Um, and instead of using one single function, I would say add few more functions and see the output quality every single time and also see how LLMs is, LMS are able to, like, make decisions. With some of you, based on how much context you are putting in the context window... And what is the context and what is the context window? If you guys have confusion in that, context window is the ability to take input in one particular request by an LM. So every LM has their own context window. It starts from... it is usually counted in the unit of tokens. Um, I think in English, like they, they have like some three to four words equivalent to one token. So you can do the math. There's a, uh, GPT tick token, uh, and there's a Python library called Tick Token also, by the way. If you wanna calculate the cost, I think tick token Python. Yeah. This is like one of the open source, uh, Python application which, which has like, um, a tokenization built in for OpenAI model. So whenever you are parsing any text, this can convert into tokens and it can tell you that these are the number of tokens you have and you can also calculate the cost. Now some of you might not need that because you can directly use, uh, if you're using like a paid API, like OpenAI API. So those features are in-built, but open source API or if you're using your own custom model or even I think the, in the Groq, um, you need to calculate the model and the request. So you can do that. It's a good practice. Um, but what I'm trying to say is that based on how many tokens you are using, your output quality will suffer. And this is a thing which we have discussed, I think, in one of the office hour discussion. And this is where, uh, the cognition report comes in, right? So this is the blog which I shared. If you're again somebody who is interested in building, uh, I would say more advanced systems, I would highly recommend you to check this article out. I'll share the link of this right now and I will also upload this on the LMS. This en- entire article is about the... from the people who are building Devin, um, which is like a AI software engineer. Supposed to be one of the best engineer who can replace the actual SDEs, but they're still like work in progress. All the very, very, very smart folks like, uh, they have defined something which is... the model is now aware of its context window because of the reasoning capability. And this was not possible with the older models. Especially they did this test with Sonnet 4.5, that we are seeing the biggest leap since Sonnet 3.6, the model which, which was used in the earlier version of Devin. The planning performance has increased. There were all the best scores right now. All of things work great, but the problem is because this time this is the most aware model Claude has ever released. The Sonnet 4.5 model, uh, is the first one we have seen that is aware of its own context window, which means it knows like how much context it has remaining. And as it approaches the context limits, we observed it proactively summarizes its progress and become more decisive about implementing fixes to close out task. This is called context anxiety, and this context anxiety can actually hurt performances. And we found the model taking shortcuts or leaving tasks incomplete when it believed it was near the end of its window, even when it had plenty of room left in. Now this is a very interesting insight and discovery which I recently inter- like learned about like models and especially when I was working with Claude, Claude Sonnet 4.5 that I experienced this by myself that... Uh, this issue happened where the researchers mentioned, uh, if you give model one million token but you just like do a system prompt, when you write a system prompt or maybe like even um, yeah, like just like in the thought process if the model thinks that it does not have one million token, irrespective of the fact whether it has or not, it still gives you the output in the same way, how a 200,000 token, uh, contest window behaves. Which means that the model's perception of how big context window is, is the, uh, main, uh, I would say driver here as compared to the actual context window size. Which is very weird because in some cases it can happen where model is like... model thinks that its capacity is to only take say two hund- two hundred words but it can actually do say, 1,000 word or maybe like more, more than that. Now this is a... This is very human-like behavior. Don't you think so where even humans have this where sometimes you don't know your potential, maybe your potential could be that you can do a lot of greater thing, you can solve much harder problems. Uh, but in your head you have told yourself that, oh, I'm not a coder. I'm not a technical person or maybe I don't know Python.... I don't write this language and hence, like, this tool calling is too difficult for me. But actually, it is not. You just need to remove that barrier from your head. Maybe just, if you look at the potential, and if you remove this entire thing, the preconceived notion that this, this is not possible, maybe then you actually find out what is the limit, right? And this is a very interesting parallel which I'm trying to draw here between the model and the human beings, which we both face the same problem of having too much on your plate and getting anxious about things, which happens to me as well. Like, whenever I have a lot of things on my plate, on, on my, uh, I would say, bandwidth gets full and I have some important event, I start forgetting things because of the context anxiety. Now, why I'm telling you all of this, suddenly in middle of this lecture, is because this is the problem which we will also see when you start implementing more and more tools. The, one of the second order effect, and, uh, uh, I would say like a con of e- implementing what we have implemented today is that all of you guys will get the different kind of output from different kind of functions. Today, it was just temperature. Tomorrow, it could be like the entire file. You know, it could be, like, maybe you're extracting information from a Google Doc which has, like, 100 pages. Now, that can fill the entire context window, and that can create the context anxiety. And if it creates the context anxiety and it fills the window, then, then your model will not behave in a way how it behaves today. And then you have a different problem, because you solved one problem which was hallucination because of lack of context. You gave it more context, now it becomes anxious and you have a different problem. Very, very important part to understand. And even if you s-, like say, uh, don't face this problem, the other problem I was telling you is that sometimes models just don't think that they have enough potential, so this is the other part. All this is not limited, uh, related to the discussion which we are doing, but context anxiety become a very important part of, uh, of the app. And one of the solution to actually solve this context anxiety, which I believe Jai would be aware of this, we need to build more AI therapists. So, we need to hire a therapist for our AI and we have to pay for the sessions and we have to send our model to this therapist and, uh, yeah. And Jai is building that, so you guys can, like... Jai, I'm, I'm sending you a lot of leads. You need to give me referral discount here. Um, but yeah, that's, that's the another thing. But jo- jokes apart, like, this was not the actual solution where you design a therapist for the models, but you have to, like, help them figure out how can they manage their context, right? Uh, and this is, this is a very good problem which kind of MCP is solving, but it has not completely solved yet, and that is also a good transition for me to tell you the next part of the series and this lecture is, how do you build systems which cannot, which can, uh, avoid these kind of problems? If we learn from the yesterday's lecture, what we did is, we implemented the client-server architecture, and this is a same architecture which is there from, uh, the entire, like, last 50, 60 years of software development, right? And if you do the same part here, where we, instead of building everything, like a tool calling f- uh, functionality, where I'm just, like, adding everything as a tool, and if I had multiple tools, I just, like, end up, like, figuring out this entire logic, because my wrapper, which is this, it becomes crowded. And although we are doing more and more things, but this, this, uh, thing of, like, having everything in one single file and, like, communicating things in a particular format makes it difficult to scale. Better way, what I realized, is to solve, and this is, like very simple insight, instead of me pulling out and pulling in the context every single time when user is asking a query, because imagine user is asking one query, this is the thing which we have solved for one single query. If you have a user who is slightly cracked and he or she is sending you maybe 100 prompts every single day about the same thing, your context gonna be, like, full, and you have to pay for those tokens as well, because the developer who is building the solution, unless until you are making a paid application, and I'm not sure how many people will pay to get the information about their weather from their LLM. But even if you figure out, like, a better use case, uh, you have to, like, also make sure that, yeah, you're charging the decent amount and you have to do the cost calculation. The cost will not make sense for you, because it will increase the cost. Every single token which you consume in this API will lead to more and more cost. So, how do you optimize this part? The idea here is what we learned yesterday. You define a protocol of servers and client, just like how we have learned, uh, from this pattern. And by the way, this is not a full solution. So, just giving you a heads up, MCP hasn't solved the context anxiety problem. Context an- anxiety problem is the article which came out in, so October, uh, I think, yeah, this was the 10th or... okay, last month only. Yeah. This was the September month. So, September 29th, these guys re- released this article when the Sonnet 4.5 released. So, MCP hasn't solved this problem r- really well. They're still, like, trying to, like, figure things part out and a lot of community, uh, developers are doing this thing. But this is one of my assumption that-If we separate things out in a way which doesn't require us to remember or our model to remember each and every small detail about the client. For example, if I have to search in my Google Drive, I have to make an API call. If I a- uh, use the same approach what I'm using today to get the weather data and it will just keep putting things in my context and then only I can, like, make any sense of the data. So, can I offload this part of fetching the details again and again and maybe do some processing here itself? Now, this is where the wrapper, the another wrapper layer comes in. Just like what we did in the first part where we gave one execution layer to our LLM and whatever code we need to run or whatever functions we need to run, we just run in this layer. Can you do the same thing here? Can we maybe give this MCP layer some capabilities so that your model doesn't need to process the information by extracting and, uh, taking input into the context? And what all things could be there? First thing, I would say resources. Now, what are these resources? These could be docs, could be text material, anything which is, could be, you can say slides, text, the data part which you keep in, right? I want all of these to stay in the MCP server unless until we are solving a very specific problem. For example, again, we are answering a query which s- is about getting the weather. Only in that case, the model should go to the MCP server and take the input and, and put it here. But for everything else, even doing your basic search, maybe we can put all the docs from this API, which is a Google Drive API. So, I will put the docs, the slides, uh, maybe the spreadsheets, everything in this MCP server, and maybe I can build a simple search system also here. Like, if my client does not have that, maybe in the tool, does not have it. In this case, Google Drive does have the search function, but even if it does not have it, then we can maybe, like, do the search here. We can figure out which is the most relevant piece of information and then we can send this back to the LLM. In that way, we are not polluting the context, we are not giving it extra data. That is one of my way of looking at how MCP could solve this. It's not, again, a standard solution, but this was a thought process which I had. Maybe this is one of the way it could solve it, right? And, uh, another way which I feel is by using tools. Just like how you saw, if you use one single tool and connect it with LLMs, uh, it works fine. But if you have multiple tools, maybe a Google Drive, uh, slides, all those things, then taking the context of those tools becomes problematic. So, can we take the input from the, uh, LLM, but output can we stay, keep it here? So, output of all the tools we'll keep in the MCP server and once we do the logic, so we can write a business logic. Again, because we are adding this execution layer, which is a wrapper around this particular tool, so we can do bunch of processing here. So, we can maybe figure out what, uh, what is the most relevant part in this output rather than just figuring out, uh, the entire output here, um, and then send it back. So, you can do all those pre-processing, lot of optimization you can do based on a particular query. Uh, the last part is your prompts. Now, before this happened, all your prompts were actually a part of your model itself. For example, if I have to train it to understand something or even if I have to give it some extra context, it was always like, uh, a part of the, this particular model only. I don't have any other way where I can maybe, say, delegate it. Maybe you can, like, build multiple models and do the chaining part of it. Definitely you can do that. But, um, I'm referring to the example which we implemented. In that case, e- even the prompts we were writing and keeping in the model itself, which was getting a part of that history in the context. That is another thing which I also believe we can do, where we can keep all the prompts separate and just like I have this entire repository, if you remember, um, this is the repository for all the prompts which I'm maintaining right now. So, can we make these repositories of prompts for different MCPs or different tools? And whenever I want to use maybe, say, my AI CTO or my PRD prompt, I can just extract this prompt and then get, maybe run this in a different LLM altogether, generate the PRD process and get only the output of that process rather than me again filling the entire details to the LLM. All of these are optimization techniques which I'm thinking out loud with you guys, um, and when I was thinking out loud all, all of these things six months back also with the last cohort, this is what MCP guys did. They implemented some of these things in the official doc. So, this is the official repository of MCP. Um, they have different SDKs. I'm gonna share the Python SDK. This is called fast-mcp-server. Just like what we have Fast APIs in, in Python, they have built fa- fast-mcp, very simple, straightforward approach to build MCP. But connecting the dot of what I discussed with you guys, that problems which we had in, um, the regular tool calling which leads to context anxiety because putting extra things here, maybe we can build a wrapper layer, the execution environment where we can run those things, we can do pre-processing of a lot of those data and we can fetch the relevant pieces only. So, we can give it some resources, we can write some logic, we can add some prompts, we can add some tools.This is the same thing, they released their Python doc right now. I think this was very recently. Most of the documentation, what they are doing, um, I think a lot of them was... I don't know exactly when they updated this particular part, but this part was recently updated in- in their, uh, doc. And it will send you the exact commit. But what I'm trying to say is that, the same thing which I told you, they have mentioned in the way how MCP can, uh, use all these things. And they are thinking in the same lines where if you know the concept of REST API and how REST helped all of us to build applications on the internet by making a standard protocol, that you have these set of rules. If you want to take something from the server, uh, use the get endpoint. If you wanna create something, use the post endpoint. You wanna update something, use the update endpoint. And just like everybody following the same rule gives you the ecosystem and- and you- you have the internet ready. That's the same thing what MCP is now trying to do. Sending you the resources part. So with resources, MCPs can expose the data, but now this data could be your docs, could be string, anything, and it's just similar to like a get endpoint, what you have seen in the API lecture. This is another part which the dots are connecting. I'm not sure how many of you understand or appreciate the information what- what you have. Like now, I think after all of these parts, because I still feel like some people, uh, who- who are not able to like connect the dots, they will feel this- this thing that we have wasted time in the full stack week. But every single person, because I've been doing this from last five cohort who have completed the two-hour six-month lectures, at the end of the day, the amount of clarity these guys get and the amount of systems they can build with the confidence and the clarity is just like difference of like almost 10 to 20 X, right? Based on wh- how they were doing, even the developers, the professional developers I'm talking about, right? And it's not about that I'm teaching them something completely new. It's just like how we connect the dots of like different concepts and different world altogether and then build systems together. Because all of this, what I'm showing you right now, it was not existed or not existent in the last six, seven months. All of this entire repo is like recently like built on- on the top of it, right? Um, like a lot of the contributions. And uh, this is how the AI is moving forward, uh, where if you don't have that first principle thinking skill, I don't think you can like now navigate in this world, because otherwise you will just like keep on, um, getting overwhelmed with the new release every single time. Um, but if you know the fundamentals, you can think with the reason and then you can connect the dots. Taking some time away from the- the previous lecture, but this is the important part, what I'm trying to say. All the problems which we face with tool calling, they have added it here and they also provide the functionality to use multiple tools using the post endpoint, and then you can define the interaction using the prompt so you have the usable templates with the LMs and which can be used to create LM interaction on the MCP server itself. So you don't need to like run the same inference which you- you need to run earlier with the prompts, uh, for the tool calling approach. Let's take a look at how we can implement all these things. I have a quick, uh, server which I just created and I'll show you the process step by step for all of you. Um, if we have time, you guys can also like implement it along with me. What you need is UV, as I mentioned in the Discord as well. So this is the UV command. Mm, yeah. If you don't have UV, I'll share this doc with all of you. It's a package manager which is built by Astral. Thanks. Um, somebody asked me that why UV is better. I think I gave the answer right now, I'll just like quickly tell to everyone because some people do have this query, which is- which is genuine query because it's a new, um, framework, and this is what I mentioned, that it's much faster than the regular PIP commands. But that's not how and that's not why I am using UV right now. The reason why it works well is that it is a replacement of all of these three things. A couple of things actually. The PIP, the PIP tools, the virtual environment, and a lot of Python toolings, which it does for caching and updating things. Whenever I've installed MCP in past and I've not used UV, I've always faced dependency problems, which means like sometimes it imports a particular library, which is already in my system, but I'm using an older version of that library, but this time, it is using the latest version and I have to create a virtual environment. If I create a virtual environment right now, I have to remember that which virtual environment I have created for this particular MCP, because tomorrow, if this MCP gets an update, I have to first download the older libraries, set up the virtual e- environment which I used last time, and then keep on adding things there. It was a mess for me. I usually do all these things, but every time I used to forget what was the name of the environment which I created, what were the dependencies I installed, and then also dependency locking. If I'm building something, building an MCP, uh, that just becomes like a problem with PIP also, like where you have to manually like lock the version. But in this case, with- if you're using UV, it- it does all of these things automatically. It gives you like an overall clean and better user interface, especially with MCPs. This is what my personal experience was.I'm trying to use UV more in my personal projects also but so far because w- we have kept everything very simple in the project and the cohorts were introduced right now. But I suggest, like, if you are using it in production or any of the application in Python, UV is, is I think, in my opinion, it's, like, getting better. Like, and it's, like, giving... In most of the outputs, I'm not facing less dependency issues these days on my local machine. Just, like, a thing about UV so you guys know what's the reasoning behind using this. Let me come back to... So first thing, you have to set up... How do you set up a MCP server? So now we are building this... what we learned yesterday, that we can convert these tools and we can wrap layer around it. So this is the first step of we building this layer. Let me keep my whiteboard on the same side. Yeah. Actually, I've already done it here but I'll, I'll show you the c- the commands. So this is the first command you need to run, uv init. Uh, this will help you start a new server. If you wanna create, uh, or, like, add as, uh, UV server or UV project in an existing repository, an ex- existing, uh, folder, then you can do uv init dot. So wherever you are using this command, it will activate it. So this was... Actually, let me run this also so I can maybe, like... Hmm. This is a blank project. I don't have anything here so I'll start fresh. You guys can follow along with me also. But you need to just keep the UV installed with you. So step number one, I ho- I ho- I'm starting things in a new project. If you have not created a new project, you can use this command. And you can write the folder name. It will generate that folder with the project. In this case, I am... I have already created a folder so I'll write this command with dot. So it will initialize this into a UV project which is a simple way to set up my Python project, give me a read me file, main.PY file. Um, this is there. So step number one done and I'll keep this here. Second thing, I need to add this uv add mcpcli. Now this will help you to add all the project dependencies. If you have anything else apart from this when you're building, like, more complex MCPs, you can a- add all the other repositories. Just like what we do pip install, just, like, a pip install version of UV. So you install all the dependency and you see, like, while I'm doing this, it has created a new environment for me. I don't need to write those virtual environment and activate those environments. So those kind of things also helps a lot. And I need to remember also the name of the environment. I know e- every time I switch to this project, I run UV command. It just takes care of everything else. Um, then, yeah, you just run this uv run mcp. This was the last command. So your MCP, uh, um, server is... Yeah, it basically saying, I think, so far things have done... Let me see if I can... Yeah, next, next up I need to install the MCP client app in order for me to talk to the server because this was a simple part where we have a Python file which is a very simple, uh, MCP server. It does not do anything. Again, a function is there. Um, and there are three commands you need to run. You need the project, add the mcpcli as a dependency, and run the project. Three steps, you can run your basic project. After that, you can add this part as a template and I've already done this so I'll show you in that folder. You just copy-paste this entire file. It has a tool, a resource, and a prompt. Okay. So you can run this part and, and, and, uh... Like, this, this part I ran last time but it was, yeah, it was like too simple of a demo for me to show. So what I did is I created a simple to-do application. Maybe you can use it to apply your current knowledge, what you did here in the lecture series, create, read, update, delete. I'm gonna use that example right now so... And I'm gonna share you... So this is the main.PY file, similar to what we got from the last exercise. This is where I think you all can be, just u- after writing those three commands. Um, actually two commands only. And then you can download the Claude and, and then you can, like, uh, see in, in that... Uh, I'll tell you, like, how to connect this with the Claude client but first let me tell you the project which I actually use and I will be using here.I just, like, keep this here. Yeah. So this is a package, the fastmcp. First, we import this and we create the mcp. This is the mcp creation step number one. You define the name. Now, this name will help you, yeah, remember what is this mcp about. Now, as I mentioned on the whiteboard itself, uh, if you remember... Mm. I'll keep it here, yeah. So we, we need resources because this, this takes a lot of the context in, in the LLM and creates that anxiety. So step number one, I'm gonna create a file and this file will stay on my PC. It will not go to LLM unless until LLM will ask for it or ask to summarize all these things. I'll just keep this file here. I was telling this in the office as well that this approach works in a lot of other ways, where even if you're building a, a project and you want to keep your context anxiety low. So instead of asking Cursor or Cloud Koda or whatever LLM you are using, that just do all the front end, back end and, uh, so database together, you keep different chats and then you create a specification for each and every part. For front end, you create a front end spec, which gives you a technology agnostic framework, which means like, for example, if I'm building a front end, it tells me, okay, what are the screens I do need to add here? What are the buttons? What are the functions to these buttons? And all of these things in natural language or a pseudo-code language, which is not tied to one particular framework or technology. Now, why th- this is helpful? Because it solves two problem. One, if tomorrow, I think Umang mentioned this, that if you're transitioning from one framework to another, which means you had started things with level one or level two, which is maybe Grady or Python, and then now you want to switch to JS, level three, you can still use your same spec- specs and ask any model to, uh, convert the code into, uh, particular technology. If you have the specs ready and all of those things are correctly, uh, detailed, uh, and docu- documented in a detailed manner, then you will find things getting easier. If you don't have those things documented well, you'll always face the issues where LLM hallucinate a lot and then you have to deal with all those problems. That, that is one thing which really helps. And another thing is that if you're sharing the context between, say, you have built a front end, you have built a back end and a database, now the database chat needs to know what is there in front end. So instead of copy pasting the entire chat of the front end, you can just ask it to look at this file which talks about the specification of front end. So figure out whatever you need from here and then you can, like, continue with your task. In that way, you can keep the context of all the, these three development chats separate and, uh, yeah. That's like a quick hack and quick tip which I was discussing in the office, I thought I'll share with everyone else also here. But that's the same concept which I'm taking here, is creating a file which is staying on my PC, um, calling it notes.txt and then this command, this function is about just making sure file is there. If file does not exist, we'll create a file. Now, this is where the MCP implementation starts so I'll just first, uh, compress all the things. So we have two tools which we are using here. We have one resource, which is the notes, and then one prompt. So I'm gonna use all the things which MCP provides me today. Yep, the resources, tools and prompts. So, uh, this resource is like a get request. As I mentioned, you can, like, take anything from here and, uh, this is where you define the prompt. Surprisingly, when I was working with MCP, I realized that because these MCPs are going to be used by both LLMs and agents, um, and when an agent will look at it, instead of reading the code, they actually read the paradox here. This is called your, uh, function docs or, or the, uh-huh, actually function docs only. Uh, yeah, instead of, like, uh, an AI l- learning through the entire part of a code, e- or extracting it from the, you can say, particular f- logic, it goes and checks this from the natural language prompt or the comment you have written. This connects with the same hack which I was telling you guys, that if you do this, your program will become more agnostic. And even the MCP client which you are using, say if that client has an LLM that does not understand the language which you are using in order for you to write these things, because MCP is not forcing a technology, it is setting up a standard. So this standard tells everyone that if you define anything as a resource, you can a- access this to, from one client to a server. Now, the implementation depends on developer and anyone can implement any language. So I can do it with Python, you can do it with JS, tomorrow someone can do it with Rust, maybe even a new language. And because the smarter decision what, uh, most MCP community developers have done is that instead of depending on the implementation, they check the comments and the docs. And because the docs are always gonna be natural language and LLMs can under- understand natural language better, it's always gonna be easier, uh, to port this, uh, entire, entire code. That was a quick thing about this because it's, I find it very fascinating that LLMs now started reading, and agents are reading more comments than human beings. This was supposed to...... I would say read by human beings, at least back then when I started software engineering. But this is the ma- main process. So we get this function. So this is a function which will get the latest note. Uh, first make sure that in- ensure the file exists. If yes, we are doing a simple Python function which opens the file, it reads all the lines, and then it, it strips the line and then give it back. If there are no lines there then it just gives that no notes yet. So simple, uh, input and output, which is a number of lines. Another tool it has, this is reading the note. Again, the same part. It starts reading everything step by step. Um, this is adding the note because you can, uh, read the note here but you need to add- make changes so you need to use the right function. Right now these things might look like, uh, very trivial to most of you or con- confusing to people who have not done the file operation in the Python, but you can replace this logic with your database credit logic. That's why I'm telling you from the credit analogy. You can use a Supabase, you can, uh, write the logic of how do you add things to Supabase or create logic and put it here, and this function will still work the same way. Again, the beauty of standard and the protocols not depending on technology. And in this case, reading a note, you can change this to your, uh, you can get API request and you can take the information from Supabase or any other data sources which you have, and you can, like, return the output from that. Um, and then finally they have the prompt which is just a simple prompt, by the way. Nothing fancy. Um, and this is a prompt. Summarize the current notes and it takes the content of the notes. So I can add maybe, like, a better prompt template here. So whatever prompt template I will add, whenever somebody is using it or maybe, uh, the person who will be accessing this MCP, they want to execute this prompt, now you can connect the notes with the PRD prompt. Most of you have faced this problem, or even the OPT prompt as well, that you have an idea. Um, now instead of you running that PRD process by yourself, you can actually delegate this to MCP server and even the MCP server doesn't need to give you all the context, uh, of the conversation. It can just follow the prompt, maybe a answer these question just like an agent, and, uh, these, these question agents or these answers could be come from, could come from maybe your notes, what you have written somewhere or maybe you have a knowledge base you've connected to it, and then it can trigger the prompt, read your data and get the entire PRD done for you, and then send this PRD to your LM or maybe whoever is calling, could be the MCP client. So that's, that's something that's a nice use case which I was thinking maybe I can build next. But these are the main functions, and this is how easy it is. You define the function for each and every part, mention the input, mention the output, add the doc. Docs are very important here, this is not optional. You have to write docs. If you don't write docs, your MCP will not behave as, as per the expect- ex- extra expected behavior. Uh, and then you can write any logic here in any language. Once you have this, uh, what's the step? Your server is running, you can run the same command you would, um, you would run MCP server. And once you run this particular server, uh, let me... I think I already installed it but I'll still show you guys. So this is the command you need to write. Once you build any, uh, MCP you have to install this MCP in your local client. If you are using Cloud Code you can just directly do this uv run mcp install and you can write the name of this file name, mean.py. So it will give you this type of response, added server AI sticky notes, so this is the name of the MCP server, to the cloud config and successfully install this in the cloud app. So once you get this information it means the MCP server has been successfully installed in your cloud client application. So we have now done these two process. We have created the MCP server around our tool. Now this time our tool is not very fancy tool but just a drive. Again, this is a template repository. I'll post it on GitHub and share with everyone. You can just change the tool call and the function call with the same logic how you have done the previous exercise and you can create this MCP server for anything in the world. And once you do this you, uh, you finalize a client. In most of the cases I would recommend you to choose the cloud desktop app because it, it is one of the most, uh, I would say easier way to test MCPs. If some of you are interested in automating your development workflow you can also use Cursor. Cursor is a great MCP client and supports a lot of tools. But for this use case I would prefer you to use cloud app. Um, let's download the cloud app right now. Enough of the MCP server, move to the clients. I'll go to cloud desktop, open this. Now first stop, first step for all of you, uh, open the settings of this application. In the settings part go to developer and this is where you can see all the MCP servers. Now as you can see on my screen I have ElevenLabs, the filesystem, the notes. Um, now this is the MCP server which we have built. I've mentioned notes MCP.Uh, by the way, like if it is not there for some of you guys, m- this is a problem, which means like your connection has not been done. Uh, you can also manually add MCPs. This is what I was telling yesterday. If you click on the Edit Config here and open this file, which is claude-desktop-config.json, you can see all the MCPs here. So as you can see, the notes MCP is here. The AI sticky notes is also here. Um, and, uh, well, I have two MCPs with just the same name. Okay. I'm not sure if this would be... Let's see if this would be a problem or not. But yeah, you can see all the pr- all the MCPs here. If I want to add a new MCP, just like ElevenLabs, you can copy the JSON file here, whatever it needs, and add it here. (sniffs) In... Let me see if I can quit my Claude and then I can run, because I think I should be able to see the new MCP also, which I... Sometimes Claude desktop app doesn't refresh by itself, so you have to close the instance and re-run it again. If you are on Windows, I'm not sure what's the equivalent of that, but, uh, you need to maybe see the Task Manager and delete from the Task Manager. Uh, not del- but delete the process from the Task Manager. Just make sure the Claude app is completely restarted. That's the thing which you need to do. Okay, now I can see my AI sticky note. Yeah, this is what I was telling you, like sometimes it doesn't refresh. So whenever you add new MCP, make sure you, uh, kill the app process and open it again. Now you will see the running sign. This is, this means everything has connected, everything has been working so far. Let's quickly test. Um, hmm, can you add MCP as the interface? Actually, MCP is the... This is my tweet of the day. MCP is the API for AGI. How many of you understand this part or can you explain this? Now, interesting part hap- happens here that this is where the tool calling comes in. So you can see it's actually using the tool calling mechanism under the hood, but because everything is standardized in a process that it knows how to talk to your client, how to talk to your server, I don't have to write any integration code specifically for my client, because Claude has all the information about how to use structured input, structured output, and everything is like very well defined. So if I build one server and I, I use that server file and follow the same process which I followed right now, I can connect it to any client. Like I can use it in my cursor. Anything which supports MCP client, I can start using my MCP server. That solves that m end-to-end problem also, which I was just talking yesterday. So I'll give it, allow once, and uh, done. I have added the MCP as the API for AGI in your sticky notes. Now, if you wanna take the resources... And by the way, I, I missed one step, sorry, uh, because it was already added. In your case, you need to click on this plus icon and you need to see the MCP. For example, in this case, I have, uh... Which MCP it used by the way? I think it used, uh, okay, I have two notes app by the way. So I'm just slightly... Okay, this use the N. N is the notes application. This one. Okay, so you can see the, I have two option. I can get the latest note. So if I click on this, this is the same function. Do you remember this function? Uh, get latest note. So this is a function which we gave it as a resource. So I can see your note MCP is the API for AGI has been saved. Is there anything else you want to do? So it just like helps me read that data what is there. Now this is the file which is on my system. This is a note .TXT and you can see I have two entries of this. Um, the second thing which I can do also from here is get the note summary prompt. Now what is this summary prompt? This is the same prompt which we have used in the last part. So this is the prompt. So I can access all these things from my, uh, I would say client. So we tested out how to add things using the add note. Whenever you, as, as you are, um, as you can see that I was doing natural language conversation, "Can you add this particular thing to my doc?" And even I, I didn't mention that particular MCP name. It just knows that these are the MCPs which I have. Another good practice is, uh, whenever you're working with multiple MCPs, um, I have realized that your, um, app again performs better if you give it specific direction. So in this case, if I only want to use one particular MCP, I can turn this on and turn off the other ones. Um, but yeah, that's the another thing which, uh, you can also do. Just make sure your, uh, MCP is turned on. The symbol is on here. And yeah. That's like a very simple implementation of MCP server, but I know like, uh, it was not very, very difficult, but I have, I chose this example intentionally so that everyone can start using this as their template repository, and you can s- add things on top of it and you can build all the MCP servers functionalities.The other main things which we had, whi- it ha- it has. Just to summarize, all the MCPs, they need a client and server. In this case, we're using Claude app as a client and because we just implemented the tool calling functionality and we realized the context pollution, which can create context anxiety, become a issue. So the same approach what we learned yesterday, how we wrap our LLM, can we wrap our tool? And we did wrap our tool around the MCP layer. Now, this MCP layer is our fast AP- fast MCP repository, which we have here on GitHub. So this repository helps us to add the layering to anything. And how does it help in terms of adding the layer? It gives us these three options. Um, these three options are the resources, the tools and the prompts. So think of resources just like a get endpoints. Whenever you want to fetch information from any resources, any documentation, or even from anything which is there on the MCP server, you can, you can use the resources part for that. Uh, how do you use that? You add resources, uh, like this. By adding resources like this and mentioning the file name, like what is the resource you're trying to access, it can point towards that. And then you can, uh, write your logic here. How do you want to read this file, write this file? Is it on database? Then you can call the database and return the output. So whatever you return here, it will, uh, take that input. And the second part here is the tools. Just like what we integrated like one single tool, you can have multiple tools here. And again, because this is not polluting the context of your main LLM, everything is going to be running on the MCP server. So you can, like, uh, define tools here. Ideally, I would say don't add, like, multiple tools even on the MCP server. Keep it like two, three for one functionality. For example, I was building the tool calling, um... sorry, the notes application. So my major reg- agenda was to... yeah, to add the notes and read the notes so I added these two tools and it works perfectly fine. Maybe you can add, like, one to add, one to read, one to update also. Um, so create, read, update, delete. Just four things, uh, I would say. I still keep this as a standard right now. I'm not sure if this is standard in the community, but if you try to keep it under four tools, you will see, like, a better performance based on my experiences or what I have also seen. Um, so yeah, this is how you can, like, start architecting your MCP resources, tools and prompt. Now, prompts are just reusable templates, just like I was telling you about PRD thing, uh, as an experiment. Instead of you generating PRD by accessing the prompt and running the entire series, you can use these prompts and generate it from a different server or maybe in the MCP server itself because this is just a Python file, which is an execution environment similar to what you had here also. So the same thing what you used here in this file, we just created the same part for the MCP also and we have a Python file. And, uh, by the way, we didn't add, like, anything specific here in this... in our MCP example. This became... Let me put this clearly so I think we'll be able to map things visually also. So this became our MCP server. Actually, I'll keep this here. This would be your tool and this would be your execution environment. Now, tool could be anything. In my case, my tool was just a notes app, and this notes app was actually a file. But you can use any tool here. Uh, so that logic will go here and then you wrap things out. So your core tool does not stay in the MCP, just like how you talk to any other, uh, tool as an API, you talk to your tool also as an API in the MCP server. Um, so yeah, that's basically the, the thing here but you can do all the processing like storing the prompts, uh, reading things, writing the logic, how do you extract information from this particular tool. Maybe you can also here call an LM and add the LM layer in the MCP also, just to run those series. So you can do all sort of things. So kind of a same version, but we are using it to solve a different problem and that's what, what, that's what's, uh, I would say the beauty of this architecture, that it's so simple in nature, but it solves like so many complicated problems which from a very long time, we were not able to solve in this entire, uh, say LM development. So that's, that's what we have done so far. If you have any questions, let me know in the chat. I think this was the end of the lecture which I had planned for you. So... I have couple of MCP also which I want to share, actually use cases. Maybe I'll, I'll create a video about this but I think that will take slightly longer or if you guys have some time, I think I can... happy to share, maybe spend one, ten minutes extra. Okay, let me get some questions first and then I'll move to the use cases. How is using MCP for resources different from RAG? It is not. It is not very different. I'll tell you and I think you will get this clarity in image once we learn about RAG. So I think you have to just wait for the next, next week lecture.By the way, next week we are doing hackathon. I didn't announce it. I think I should announce it. So we are pla- coming up with the hackathon and, uh, we'll officially announce it on the Discord as well. But so next week we planned that whatever you guys have learned, because you've learned a lot, it's a test time, um, so we'll see like how many of you are able to apply the knowledge, what you have learned. So yeah, just before we have our Diwali holidays, we're gonna do a mini-hackathon and we'll have some in- interesting prizes for all of you as well. So yes, yeah, make sure to participate and finish all the lectures before that and just prepare for it. But yeah, j- just to answer your question, Nimesh, it is not very different. Like some part of it is similar. The retrieval part is like similar, um, and it's like the same evolution what we have seen in the databases. If you remember how I taught you database from the pre-recorded example of, uh, lead qualification where we first used in-memory database, which is everything inside one variable function or in a v- one variable of Python. Then we moved to level two where we started using file-based data storage, where we had CSV file and we used that. And then level three was our super base where we started using a database system. That is the same analogy I can use here in the RAG, where what I'm doing with tool calling and MCPs and accessing these resources, they come under those file-based systems for your, you can say, like a level one database solution, but RAG is more like a level two or level three solution, which is like more sophisticated way of dealing with files because once, once you scale things up, this system doesn't, uh, scale very well. I hope that answers, that will answer your question. But I'll talk more about it in the RAG lecture. Um, is it running? Okay, let me s- Ashwin, do you wanna share your screen? We can actually debug this right now. Let me see. Let's do a one quick debugging for everybody. Uh, can you hear me, Ashwin? Yeah. Hello. Now audio? Yeah. Okay. So, uh, should I share my screen? Yeah. Can you share your screen? Let's see if we can debug your issue. I followed these steps, uh, but, uh, this, uh, run command, I don't know if it's working or not. Um, did you... Okay, I think I, I know the issue actually, but let me see your Claude. Can you open the Claude desktop app? I don't have Claude. So how, how will you test? Uh, so you were saying that, uh, we can test it with Cursor also? Yeah, we, we can, um, but the only thing is, um, I don't have like exact steps. How can I verify it? Because sometimes even for me, like Cursor throws some weird issues, which I even I have not resolved right now. I would say if you want to test it out, the easier way is to use Claude. Once it works for Claude, then you can like move to Cursor because I know there are cases where I have used a particular MCP and that was not working for Cursor, but it was working for Claude. So those kind of issues can arise. Although in this case it should not, but, um, I would say- But like here, uh, it sh- uh, the server should start first and then we, uh, need to connect with Claude code, right? How do I know that this MCP server is running or not? So MCP server will run locally when your client is activated. As of now, like if you c- if you wanna do the run, you can use the dev command which it is showing, that run an MCP server within MCP inspector. Now this is one of the version how you can run it. Uh, can you do the same command? Uh, uv run mcp dev. Actually, I think you can ask Cursor also if this command is correct or not. But the file spec is missing. Can you run, uh, MCP, UV run MCP install, the command which we ran, uh, for the program? Let me give you the exact command, I think. Hmm, this was... Uh, this one? Yeah, UV run, uh, MCP install, and then file name. This is... Which file name should I mention here? The file name which you have, main.PY. But again, you need Claude desktop application here. Okay. It won't, it won't do that. Yeah, I think, so my assumption right now, again, I could be wrong here, is that the MCP client which is running, the, the job of that client is also to execute the code in the c- in the server. So when, when you have a client running on your PC-... that can call your code. And essentially, it is just like calling the code, right? So that's, that's where the server should run. You can run mcp in a dev server. This is what the dev command does. I don't know exact command. I think they have command on the, uh, GitHub repository. Can you go to GitHub repository? Yeah. This is the command, uv run mcp dev. Can you run this once again? And just mention the main .py file? Hmm. Yeah, so this is the same, because you don't have a server object right now. So you need to copy the code which it was, it, uh, which was there in the GitHub repository. Yeah. You can copy this entire code and paste it here. Can you run again? Okay. I think now it's running, so you need to install this. So is it like mcp servers only, uh, can be used with Claude and, uh, without that, uh, we cannot use mcp servers? You need a client now in order for you to use any kind of server. So like, you need a browser for you to even test and build your websites and your applications, because you will- Yeah, but, uh, is there any open source method, uh, or a client that we can use for this purpose? Because I don't have Claude subscription right now. I only have OpenAI. You don't need, you don't need to buy Claude, by the way. You can use free version of Claude and you can still do the sa- same thing. Uh, Claude code, but only comes with subscription. Not Claude code. I'm saying Claude desktop app, Ashwin. Oh, okay. I didn't know that those two were different things. Yeah, yeah. Those, those are different things. So download the Claude desktop app. It's like free, you don't need to pay anything for that. And this is another way what you're saying, you can use the mcp inspector. But okay- Okay. ... it won't make any sense. It's just like a debugger, so... Oh, okay. Okay. Yeah. You need a client. Oh, okay. Got it. Thank you. Cool. Aw- awesome. Yeah, Manoj. I think you have raised your hand. (laughs) Yeah, can you- Can you hear me? I can hear you. Yeah. Uh, just a quick question. Uh, from when we switched from tooling to the mcp server, right? And we talked about context positioning, right? So just in a simple way to understand, we are offloading the work of prompt generation or summarization to an external client which is doing the work and not polluting the context? Yeah. Like even it could be summarizing and all that stuff? Yeah. That is one, one of the way how we can offload the work and offload the context also. Okay. So that is- Uh, is it, is it- ... 100% what we see in- ... happening with the mcp server or is it something different? You have to write that logic, but you can do all those things. You can run a code just like what I told you. You can add a prompt, you can add a tool, you can add a resource, and you can use these things to... It's a, it's a Python file. So whatever you can do in a Python file, you can do in that mcp server. So it's not nothing different from your classic server as well. Okay. Okay. And, uh, would you be able to share some real name example where it has proven to be... Like, I just want to see. I'm not able to make sense of when, when it will pollute the environment and we are starting to see those in a real scenario. I think you should try it. Why don't you try it? Um, just ha- have you attempted the tool calling exercise? Yeah, yeah. So just as a- Just a context to do is not that big enough for me to test. Maybe I'll give it a try. I was just trying to see any real scenario in production. I'll, I'll, I'll tell you- Okay. ... like a situation how you can try it. Uh, connect it with your, maybe Google Doc or maybe even notes, just like what we made a notes application. Connect it with a file, uh, and then add like maybe- Yeah. ... 100,000 pages or like add a PDF book, e-book, upload that e-book, or maybe like copy paste the e-book content in this file, and then use a tool called... and then ask it to extract the information from this file. I think then you can see the difference. Very simple way to do it. Okay. Yeah. Okay. Okay. Thank you. Awesome. Um, yeah, Vishnu? Hi, Siddharth. Uh, last time you told about, right, like, building an AI version of yourself as a tutor so that it can be anywhere any time. So I was just thinking and the only problem I thought was, you know, memory is kind of context as we discussed today as well, that it is limited, and Claude as we code as well, you know, it, it, um, like compresses whatever it has done and then creates like a new context after your credits are kind of... After a bit, it, it becomes kind of very lengthy. But I, I was just like looking at, you know, super memory and then there's some also open source, uh, tools that are now kind of coming as memory. Do you think that can help? And if yes, like how, at what stage are they for, for memory purpose?Yeah, I think we- we have not, uh, learned about the memory as a concept right now. This is the second part of augmented LLM. So actually, like, third part. So we started with tool calling in this week. Next week, we're gonna move to RAG so we learn how to retrieve and, again, this was a level two of what we were doing today, is, like, just reading from the file and then, like, learning about the retrieval system and building those things. And once you learned these two concepts, then I think a lot of things will get clear to you, that why and how you can solve the memory problem. And that is also the last module of the augmented LLM, uh, which is, uh, the no- the, uh, memory system. How do you build that? So there's a lot of open research, Vishnu, there. The same problem what you are facing is- has been faced by, like, a lot of developers and- and they're still facing it. Uh, the super memory which you spoke about is an open source solution built by, I think, Dhairav? Dhairya or- Yeah. ... yeah. What- the guy, he's, like, uh, quite young right now. And- and- and I think, uh, those kind of solutions are there. I s- uh, used MemGPT and Mem0. These were the two, another memories- Mem0 is open source. Right? Mem0 is open source. Yeah. I think Mem0 is the one which I used. MemGPT was not. Um... Yeah. There are s- s- uh, some solutions which are out there in the open source market, but I can't give you, like, one solution because of the nature of problems are very different. For example, the same way how I cannot tell you that this is the text tag and- and the architecture works for all the problems in the software. Every software have a different input, different output and different logic based on your understanding and what are the things you know. So even for one problem statement, my input could be same, but based on my understanding and your understanding, we too can decide and design a two different architecture altogether, right? Same way, in the LLM space, um, we have multiple options. Like, you could use super memory, you could use, like, RAG, you could use the classical nice solution of context engineering, what we are trying to do where you keep everything in files and extract information. Different solutions. You can choose all these things and- and build a solution for it. What is the problem statement here? If the problem statement is to build an AI instructor, then we'll start with the step number one. What is the input we need? What are the data sources we need? Can we organize this data source in a way which would be easier for an LLM to understand based on the queries? And how should we organize? What should be the decision? And the framework for us to organize is the kind of queries which we receive. So you describe all the queries, you reverse engineer your queries, see the output you expect from those answers and then- then decide the same logic. Exactly what I did in the MVP lecture. Okay, what would be the best workflow for us to extract the information from a knowledge base to give this kind of output? So this is the short answer which I can give you. But we have entire lecture which I'll, uh, maybe dedicate to this topic of memory and retrievals. I hope that makes sense. Yeah. I mean, it makes me more excited for the next one, definitely. Cool. Awesome. Thanks. Mm-hmm. Thanks. The buildathon is in Diwali week or after that? Uh, Diwali is actually 21st, right? If I'm not wrong. Uh, so it's, like, before that. 17th or 18th. Let me tell you, I think, exactly the dates. Uh, 17th and 18th. So we're gonna start 17th. You will receive your problem statements and, uh, 18th we're gonna end. So it's gonna be a 24-hour journey. So yeah. What else can be Cursor? Uh, Claude cl- uh, sorry. What else can be client? If you want to check out the client list, this is a question which you asked so I'll share the resource. This is the official model context protocol repository. If you wanna look at clients, this is for J. Just go and check out the clients here. There are a lot of clients which you could use. I think they have mentioned MCP... They keep on changing their docs. I think they have... Let me search. MCP. Yeah. The example clients. Sorry, this was the one. So these are the- some of the popular clients. And as you can see, there are different things which are available for all the clients. Resources, prompts, tool, discovery. There are some new things which I haven't used. Sampling, roots and elicitation. Um, and you can see, like, Amazon Q, Amp, Apify, MCP tester, Bolt.ai, all these are clients. Claude Core is also a client. Claude AI is also a client. CodeGPT is a client. Cursor is a client and, uh, uh, selling a thing. Now Cursor supports all three. Earlier, they- they would support only the resources part. Actually, only- only- only the tools part. But, uh, yeah, I think... Yeah, you can see all the list here of the clients. Example clients. And then you have example servers also. I think... Yeah, these are some example servers. Not example servers, but they are actual servers. So this is an index and repository maintained by MCP, so if you are testing all the other servers, I would say first go to this before installing a random server. Just see if it is indexed by the MCP community or not. For example, you can see here, I think the ElevenLabs guys also mentioned there, uh...Yeah, 11 Labs is also there. So all the official ones are there. It's a good way for you to, um, just see what kind of official servers are there. So you can see, like, a lot of them. Google Ads also has the MCP server. It's a good exercise for you to go and check out the existing servers. For example, you can see the GitHub MCP. GitHub MCP is actually useful, by the way, because you can figure out a lot of s- uh, the GitHub commands and, like, you don't need to write. Every time I use Cursor for GitHub, Cursor end up, like, making a lot of decisions which it does not know because it does not have the entire context of how many branches I have, how many, uh, I would say pull requests are open, what are the issues. So if you use GitHub, GitHub can also help you to give the right context to your agent and it just, like, makes your workflow much more easier. And you can see like how... I think these guys have used the TypeScript version of it. Some MCPs are written... A- as again, I've mentioned this. MCP doesn't force, enforce the technology. It just mentions the protocol, so as long as you follow the same protocol, you can use it in TypeScript, you can use it in, uh, the language. But it's a good exercise. If you are somebody who wants to build official and advanced MCP servers, go and check out the server repository and I think you will find something interesting. And even if you don't want to use it, you can connect these servers together and build something interesting with them. Awesome. Amber. This is... I think Amber mentioned that it is working. That's great. That's great. Wow, Babajan is working. Yeah, I think now you can... guys can try in Cursor as well. Do you want to sh- share it with everyone? I think I'll... I like your output, Babajan. I think this would be a good closing part of the exercise, what we did. Uh. Let me ask you to share the screen. Can you share your screen? Yeah. Hey, Babajan. Yeah, sure. What's your nickname, Babajan? I keep on calling you full name all the time. No, this is Babajan, actually. My... First name. Baba. Okay, nice. Yeah. Do people call you Baba? (laughs) Here in Saudi Ar-... Actually I'm working in Saudi Arabia, so he- these peoples are very strict. So Babajan, actually in Arabic means, uh, dad come here. The meaning is. They always, uh, make fun of my name. (laughs) Oh, okay. (laughs) So, nice. So it is actually regional name. Uh, I'm actually from Andhra Pradesh. Oh. So it's a regional API. Interesting. (laughs) So, here, I think, uh, uh, this is the one, uh, we'll show you in the, in the Cursor. So this is a JSON format file. Mm-hmm. Yeah. So I have one here. So I just created the JSON format file in the Cloud option, cloud dot, JSON form. So when... It's work. This, uh, previously, uh, the LLN, the client I have created, the main.pv and the weather a- API. Yeah. So I directly collected from, uh, one of the resource, the open source where, uh, uh, it's having the free API key for the weather. So it's not, uh, for... Oh, I connected for open micro. Mm-hmm. It's open micro. It is having all this information about the coordinate, location, humidity, all the information is directly. Awesome. This is great, yeah. And, uh, can you show the MCP server as well? Sure. Yeah. Yeah. This one. I guess you built it and, uh, is it running on Cloud? Can you just, uh- Yeah. Yeah. Yeah. ... show us the output? Yeah, it's running on Cloud. It's right here. Oh, nice. Is it the correct temperature? Yeah, it is correct temperature. Yeah, it's 30 degrees. Awesome. You- Yeah. You solved the problem which we discussed yesterday in the beginning, so congratulations first of all. Uh, great work. Thank you. How are you feeling? I'm good. Yeah, it's, uh... I mean, I... I'm just, um, typing a message to you that now I have developed many tools for, uh, our, actually daily work, so now I can wrap this one in MCP, then I can make... it might be highly useful for our, uh, community, especially I'm working with community. It is very, very essential and useful and as you said in earlier lecture, uh, there are many peoples out there who doesn't understand the installation and all those things. Now it's easy for them. You just share them just few... give it few instruction and they can play... they can do many things with it. Awesome, yeah. This is great. Uh, I'm looking forward to see more inputs from your community. Uh, let me know. I think we can maybe have a discussion around this if you, if you want to maybe, like, have a further discussion about how we can, like, share more information. Because I, I, I think that there are a lot of people from UAE who are reaching out to me about, like, these small, small things and I thought, I don't know a lot of people there, so maybe, like, I can share the- Yeah. ... information through you. Yeah. Sure, sure. Yeah, yeah. Awesome. Okay. But thank you, Babajan, for sharing this. Cool guys, uh, I know I had a couple of questions. I'll quickly, like, take the last question, then we'll wrap this up. Can I test the MCP server without client, like an API testing in a Postman? Yes. So you have a debugger. This is exactly what I, uh, helped Ashwin to do. So if you can run the MCP using the depth command, it is there on the readme file, it will open the inspector. You can see the input and output, but, uh, again, it's gonna be always difficult for you to, uh, like, yeah, do it without the tooling. I think people are building some toolings out there, like there are people... uh, the companies who are building the observability. Entire space is very new, by the way. All of these, the development happened in last couple of months and couple of years only. Actually, couple of months. So, yeah, I'm not sure. Maybe we can find if there's a Postman equivalent for MCP as of now. I would say you can still use Curl command. Um, Curl is one way to, like, talk to MCP. Um, if you have an, an, uh, I would say CLI cloud code or maybe even a cloud desktop app, that could be one thing. Um, yeah. Awesome, guys. I'll see you next week. As I mentioned, like, next week we will not have the lecture, we'll have a hackathon because this is gonna be like a catch-up week for people who were not able to, like, follow all the lectures, so you guys can use it as a catch-up but people who have already, like, followed all the lectures, this is a great opportunity for you to showcase. Uh, we're gonna come up with some interesting problem statements and, uh, I'll share that, all of you, with you on Friday. We'll do, we'll do a 24-hour hackathon and Saturday we announce the winners and we'll do some interesting. So until then, I'll see you and wish you all the best. Bye-bye.
